{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Convolution Neural Network\n",
    "<br />\n",
    "## - Network Architecture\n",
    "###  : Input - Conv - ReLU -Pooling - Conv - ReLU -Pooling - Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "x_image= Tensor(\"Reshape_1:0\", shape=(?, 28, 1, 28), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# MNIST 데이터 생성\n",
    "# ont_hot = True는 Y의 레이블이 1이라면 [0,1,0,0,0,0,0,0,0,0] 처럼 0또는 1로 코딩해주는것\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# placeholder로 x, y값을 저장할 공간 할당\n",
    "# [None, 784]의 의미는 행의 수는 무한대로 받을 수 있다는 뜻 \n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,28, 28, 1])\n",
    "print (\"x_image=\",x_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_label_shape :  (55000, 10)\n",
      "train_image_shape :  (55000, 784)\n",
      "test_label_shape :  (10000, 10)\n",
      "test_image_shape :  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_label_shape : \",mnist.train.labels.shape)\n",
    "print(\"train_image_shape : \",mnist.train.images.shape)\n",
    "print(\"test_label_shape : \",mnist.test.labels.shape)\n",
    "print(\"test_image_shape : \",mnist.test.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.random_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 표준편차를 0.1로 가지는 가중치 함수 정의\n",
    "- 편의가 0.1인 편의 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def con2d(x, w, b):\n",
    "    layer = tf.nn.relu(tf.nn.conv2d(x, w, strides=[1,1,1,1], padding='SAME')+b)\n",
    "    return layer\n",
    "\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stride가 1이고, Padding은 0인 convolution 함수 정의\n",
    "- Stride가 2이고 filter 사이즈가 2*2인 MaxPooling 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Graph 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#Input - Conv1 - ReLU - MaxPooling - Conv2 - ReLU - MaxPooling - FC - SoftMax\n",
    "##############################################################################\n",
    "\n",
    "##########################################################\n",
    "#첫번째 Layer\n",
    "##########################################################\n",
    "# 첫번째 Layer의 Convolution Filter 설정\n",
    "# 5x5크기의 32개의 Filter 생성\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "# 32개의 bias 생성\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# 첫번째 Convolution Layer 생성\n",
    "h_conv1 = con2d(x_image, W_conv1, b_conv1)\n",
    "# Convolution Output에 대한 2x2 maxpolling\n",
    "h_pool1 = max_pool(h_conv1)\n",
    "# Input         -> 28*28\n",
    "# After Conv    -> 28*28 (5*5 필터, padding=SAME)\n",
    "# After pooling -> 14*14\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "#두번째 Layer\n",
    "##########################################################\n",
    "# 두번째 Layer의 Filter 설정, 첫번째 Layer의 Filter의 갯수를 32개로 설정하였기 때문에\n",
    "# depth는 32로 설정\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "# 두번째 Convolution Layer 생성\n",
    "# 첫번째 Layer의 최종 output인 hpoo1을 입력으로 받음\n",
    "h_conv2 = con2d(h_pool1, W_conv2, b_conv2)\n",
    "#Convolution Output에 대한 2X2 Maxpooling\n",
    "h_pool2 = max_pool(h_conv2)\n",
    "# Input         -> 14*14\n",
    "# After Conv    -> 14*14 (5*5 필터, padding=SAME)\n",
    "# After pooling -> 7*7\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "#Fully Connected Layer\n",
    "# : Classification을 위해 3차원 데이터를 \n",
    "#   1차원 tensor로 변환 후 Drop Out\n",
    "##########################################################\n",
    "# 7x7에 해당하는 Fully Connected Filter 설정\n",
    "# 출력은 1024\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# 두번째 Layer의 Output size를 통해 Flatten 작업 수행\n",
    "# Tensor to Vector\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# 드롭아웃되지 않을 확률 값을 저장할 플레이스홀더를 생성 및 Dropout Layer 추가\n",
    "# drop out : unbalanced weight node들을 삭제 -> 과적합 문제 해결방안임\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# SoftMax 계층 전에 입힐 Filter 설정\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# 1차 Fully Connectered Layer의 결과를 통해 softmax\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) #hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (Conv > ReLU > Max Pooling) > (Conv > ReLU > MaxPooling) > (Fully Connect>Drop out > SoftMax) layer 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, Training Acc 0.05\n",
      "step 10, Training Acc 0.07\n",
      "step 20, Training Acc 0.05\n",
      "step 30, Training Acc 0.11\n",
      "step 40, Training Acc 0.08\n",
      "step 50, Training Acc 0.12\n",
      "step 60, Training Acc 0.14\n",
      "step 70, Training Acc 0.14\n",
      "step 80, Training Acc 0.15\n",
      "step 90, Training Acc 0.19\n",
      "step 100, Training Acc 0.19\n",
      "step 110, Training Acc 0.25\n",
      "step 120, Training Acc 0.24\n",
      "step 130, Training Acc 0.26\n",
      "step 140, Training Acc 0.23\n",
      "step 150, Training Acc 0.3\n",
      "step 160, Training Acc 0.27\n",
      "step 170, Training Acc 0.33\n",
      "step 180, Training Acc 0.28\n",
      "step 190, Training Acc 0.4\n",
      "step 200, Training Acc 0.44\n",
      "step 210, Training Acc 0.5\n",
      "step 220, Training Acc 0.46\n",
      "step 230, Training Acc 0.45\n",
      "step 240, Training Acc 0.52\n",
      "step 250, Training Acc 0.54\n",
      "step 260, Training Acc 0.49\n",
      "step 270, Training Acc 0.38\n",
      "step 280, Training Acc 0.48\n",
      "step 290, Training Acc 0.48\n",
      "step 300, Training Acc 0.6\n",
      "step 310, Training Acc 0.58\n",
      "step 320, Training Acc 0.6\n",
      "step 330, Training Acc 0.59\n",
      "step 340, Training Acc 0.64\n",
      "step 350, Training Acc 0.61\n",
      "step 360, Training Acc 0.58\n",
      "step 370, Training Acc 0.65\n",
      "step 380, Training Acc 0.57\n",
      "step 390, Training Acc 0.69\n",
      "step 400, Training Acc 0.56\n",
      "step 410, Training Acc 0.61\n",
      "step 420, Training Acc 0.7\n",
      "step 430, Training Acc 0.66\n",
      "step 440, Training Acc 0.7\n",
      "step 450, Training Acc 0.74\n",
      "step 460, Training Acc 0.69\n",
      "step 470, Training Acc 0.76\n",
      "step 480, Training Acc 0.69\n",
      "step 490, Training Acc 0.72\n",
      "step 500, Training Acc 0.73\n",
      "step 510, Training Acc 0.69\n",
      "step 520, Training Acc 0.68\n",
      "step 530, Training Acc 0.72\n",
      "step 540, Training Acc 0.74\n",
      "step 550, Training Acc 0.77\n",
      "step 560, Training Acc 0.8\n",
      "step 570, Training Acc 0.71\n",
      "step 580, Training Acc 0.72\n",
      "step 590, Training Acc 0.77\n",
      "step 600, Training Acc 0.73\n",
      "step 610, Training Acc 0.71\n",
      "step 620, Training Acc 0.72\n",
      "step 630, Training Acc 0.72\n",
      "step 640, Training Acc 0.77\n",
      "step 650, Training Acc 0.75\n",
      "step 660, Training Acc 0.79\n",
      "step 670, Training Acc 0.85\n",
      "step 680, Training Acc 0.8\n",
      "step 690, Training Acc 0.78\n",
      "step 700, Training Acc 0.83\n",
      "step 710, Training Acc 0.74\n",
      "step 720, Training Acc 0.8\n",
      "step 730, Training Acc 0.71\n",
      "step 740, Training Acc 0.84\n",
      "step 750, Training Acc 0.89\n",
      "step 760, Training Acc 0.8\n",
      "step 770, Training Acc 0.76\n",
      "step 780, Training Acc 0.8\n",
      "step 790, Training Acc 0.83\n",
      "step 800, Training Acc 0.77\n",
      "step 810, Training Acc 0.71\n",
      "step 820, Training Acc 0.79\n",
      "step 830, Training Acc 0.75\n",
      "step 840, Training Acc 0.8\n",
      "step 850, Training Acc 0.81\n",
      "step 860, Training Acc 0.82\n",
      "step 870, Training Acc 0.76\n",
      "step 880, Training Acc 0.87\n",
      "step 890, Training Acc 0.82\n",
      "step 900, Training Acc 0.93\n",
      "step 910, Training Acc 0.85\n",
      "step 920, Training Acc 0.85\n",
      "step 930, Training Acc 0.73\n",
      "step 940, Training Acc 0.87\n",
      "step 950, Training Acc 0.89\n",
      "step 960, Training Acc 0.8\n",
      "step 970, Training Acc 0.8\n",
      "step 980, Training Acc 0.81\n",
      "step 990, Training Acc 0.81\n",
      "step 1000, Training Acc 0.77\n",
      "step 1010, Training Acc 0.79\n",
      "step 1020, Training Acc 0.83\n",
      "step 1030, Training Acc 0.85\n",
      "step 1040, Training Acc 0.91\n",
      "step 1050, Training Acc 0.84\n",
      "step 1060, Training Acc 0.88\n",
      "step 1070, Training Acc 0.85\n",
      "step 1080, Training Acc 0.85\n",
      "step 1090, Training Acc 0.85\n",
      "step 1100, Training Acc 0.85\n",
      "step 1110, Training Acc 0.95\n",
      "step 1120, Training Acc 0.88\n",
      "step 1130, Training Acc 0.85\n",
      "step 1140, Training Acc 0.87\n",
      "step 1150, Training Acc 0.85\n",
      "step 1160, Training Acc 0.85\n",
      "step 1170, Training Acc 0.89\n",
      "step 1180, Training Acc 0.84\n",
      "step 1190, Training Acc 0.88\n",
      "step 1200, Training Acc 0.8\n",
      "step 1210, Training Acc 0.86\n",
      "step 1220, Training Acc 0.88\n",
      "step 1230, Training Acc 0.85\n",
      "step 1240, Training Acc 0.81\n",
      "step 1250, Training Acc 0.91\n",
      "step 1260, Training Acc 0.89\n",
      "step 1270, Training Acc 0.87\n",
      "step 1280, Training Acc 0.81\n",
      "step 1290, Training Acc 0.92\n",
      "step 1300, Training Acc 0.89\n",
      "step 1310, Training Acc 0.86\n",
      "step 1320, Training Acc 0.79\n",
      "step 1330, Training Acc 0.87\n",
      "step 1340, Training Acc 0.88\n",
      "step 1350, Training Acc 0.92\n",
      "step 1360, Training Acc 0.87\n",
      "step 1370, Training Acc 0.9\n",
      "step 1380, Training Acc 0.84\n",
      "step 1390, Training Acc 0.87\n",
      "step 1400, Training Acc 0.85\n",
      "step 1410, Training Acc 0.87\n",
      "step 1420, Training Acc 0.9\n",
      "step 1430, Training Acc 0.94\n",
      "step 1440, Training Acc 0.89\n",
      "step 1450, Training Acc 0.86\n",
      "step 1460, Training Acc 0.85\n",
      "step 1470, Training Acc 0.91\n",
      "step 1480, Training Acc 0.87\n",
      "step 1490, Training Acc 0.83\n",
      "step 1500, Training Acc 0.83\n",
      "step 1510, Training Acc 0.9\n",
      "step 1520, Training Acc 0.81\n",
      "step 1530, Training Acc 0.88\n",
      "step 1540, Training Acc 0.92\n",
      "step 1550, Training Acc 0.92\n",
      "step 1560, Training Acc 0.9\n",
      "step 1570, Training Acc 0.91\n",
      "step 1580, Training Acc 0.85\n",
      "step 1590, Training Acc 0.9\n",
      "step 1600, Training Acc 0.91\n",
      "step 1610, Training Acc 0.9\n",
      "step 1620, Training Acc 0.88\n",
      "step 1630, Training Acc 0.92\n",
      "step 1640, Training Acc 0.9\n",
      "step 1650, Training Acc 0.93\n",
      "step 1660, Training Acc 0.86\n",
      "step 1670, Training Acc 0.9\n",
      "step 1680, Training Acc 0.94\n",
      "step 1690, Training Acc 0.95\n",
      "step 1700, Training Acc 0.88\n",
      "step 1710, Training Acc 0.92\n",
      "step 1720, Training Acc 0.94\n",
      "step 1730, Training Acc 0.9\n",
      "step 1740, Training Acc 0.92\n",
      "step 1750, Training Acc 0.86\n",
      "step 1760, Training Acc 0.9\n",
      "step 1770, Training Acc 0.91\n",
      "step 1780, Training Acc 0.92\n",
      "step 1790, Training Acc 0.82\n",
      "step 1800, Training Acc 0.92\n",
      "step 1810, Training Acc 0.88\n",
      "step 1820, Training Acc 0.94\n",
      "step 1830, Training Acc 0.89\n",
      "step 1840, Training Acc 0.94\n",
      "step 1850, Training Acc 0.94\n",
      "step 1860, Training Acc 0.96\n",
      "step 1870, Training Acc 0.94\n",
      "step 1880, Training Acc 0.92\n",
      "step 1890, Training Acc 0.92\n",
      "step 1900, Training Acc 0.94\n",
      "step 1910, Training Acc 0.87\n",
      "step 1920, Training Acc 0.83\n",
      "step 1930, Training Acc 0.9\n",
      "step 1940, Training Acc 0.88\n",
      "step 1950, Training Acc 0.86\n",
      "step 1960, Training Acc 0.93\n",
      "step 1970, Training Acc 0.93\n",
      "step 1980, Training Acc 0.88\n",
      "step 1990, Training Acc 0.9\n",
      "Test Acc 0.9156\n"
     ]
    }
   ],
   "source": [
    "#초기 설정\n",
    "learning_rate = 1e-4\n",
    "batch_size = 100\n",
    "\n",
    "# model 훈련 및 평가\n",
    "# cost_fn\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "#accuracy op 생성\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "# 모든 변수 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# 1스텝 당 100개씩 2천번 학습\n",
    "for i in range(2000):\n",
    "    batch = mnist.train.next_batch(batch_size)\n",
    "    if i%10 == 0:\n",
    "        result = sess.run(accuracy, feed_dict={ x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, Training Acc %g\"%(i, result))\n",
    "        sess.run(optimizer,feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"Test Acc %g\"% sess.run(accuracy, feed_dict={\n",
    "       x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9156\n"
     ]
    }
   ],
   "source": [
    "# Testset accuracy 계산\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값:  [5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9804f936d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 랜덤하게 이미지를 뽑아서 이미지 인식 테스트 수행\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "\n",
    "# 해당 이미지 및 예측값\n",
    "\n",
    "print(\"예측값: \", sess.run(tf.argmax(y_conv, 1), feed_dict={x: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정답:  [5]\n"
     ]
    }
   ],
   "source": [
    "# 정답\n",
    "print(\"정답: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
